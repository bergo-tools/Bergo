# Bergo 限流配置示例
# 这个文件展示了如何为LLM请求配置限流功能

# 基本配置
debug = false
language = "chinese"
line_budget = 1000
compact_threshold = 0.8

# 模型配置 - 包含限流设置
main_model = "deepseek-chat"

[[models]]
identifier = "deepseek-chat"
provider = "deepseek"
model_name = "deepseek-chat"
base_url = "https://api.deepseek.com/beta"
temperature = 0.7
max_tokens = 8192
prefill = true
context_window = 131072
# 限流配置：每次请求间隔1秒，防止API限流
rate_limit_interval = 1.0

[[models]]
identifier = "deepseek-reasoner"
provider = "deepseek"
model_name = "deepseek-reasoner"
base_url = "https://api.deepseek.com/beta"
temperature = 0.7
max_tokens = 65536
context_window = 131072
# 这个模型没有设置限流，会按正常速度请求

[[models]]
identifier = "gpt-4"
provider = "openai"
model_name = "gpt-4"
base_url = "https://api.openai.com/v1"
temperature = 0.7
max_tokens = 4096
# OpenAI有严格的限流要求，设置为2秒间隔
rate_limit_interval = 2.0

[[models]]
identifier = "claude-3"
provider = "anthropic"
model_name = "claude-3-5-sonnet-20241022"
temperature = 0.7
max_tokens = 8192
# Anthropic的限流配置
rate_limit_interval = 1.5

# API密钥配置
deepseek_api_key = "your-deepseek-key"
openai_api_key = "your-openai-key"
anthropic_api_key = "your-anthropic-key"

# 使用说明：
# 1. rate_limit_interval 的单位是秒
# 2. 值为 0 或不设置表示不限流
# 3. 值大于 0 会在每次请求前检查时间间隔
# 4. 限流是按模型标识符隔离的，不同模型互不影响
# 5. 限流逻辑在 NewLlmStreamer 函数中自动应用